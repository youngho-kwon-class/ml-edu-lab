AdamP vs Adam Optimizer Benchmark âœ¨
This project was created to compare the performance of two widely-used optimization algorithms in deep learning: Adam and AdamP. It benchmarks training efficiency, convergence speed, and final accuracy using the FashionMNIST and CIFAR-10 datasets.

ðŸ“‚ Project Structure

cifar10/: Contains the code to compare the performance of Adam and AdamP on the CIFAR-10 dataset.

fashionmnist/: Contains the code to compare the performance of Adam and AdamP on the FashionMNIST dataset.

ðŸ’¡ Key Concepts
Adam (Adaptive Moment Estimation): A highly efficient and widely-used optimization algorithm that combines the advantages of Momentum and RMSprop, dynamically adjusting the learning rate for each parameter during training.

AdamP (Adam with Projection): A variant of Adam that introduces a "projection" concept to the weight update rule. It aims to improve convergence speed and the model's generalization performance.

ðŸš€ How to Run the Code

Navigate to either the cifar10/ or fashionmnist/ folder.
Execute the cells in order as instructed within the notebook.
