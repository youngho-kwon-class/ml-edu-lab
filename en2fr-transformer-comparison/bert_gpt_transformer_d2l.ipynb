{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzp41SMTagQ0ucqkuPeWmg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ›  BERT+GPT Hybrid Transformer (D2L Transformer Chapter Pattern)\n",
        "\n",
        "This notebook is based on the **Transformer Encoder/Decoder pattern** and **PositionalEncoding**  \n",
        "introduced in the D2L \"Transformer\" chapter, but reconfigured into a **BERT-style encoder**  \n",
        "and a **GPT-style decoder**.\n",
        "\n",
        "> âš  **Important:**  \n",
        "> - Because the dataset is very small and the number of training epochs is limited, the actual translation quality will be poor.  \n",
        "> - The purpose is to understand the architecture and training procedure, not to achieve production-quality translation.  \n",
        "> - High-quality translation would require a much larger dataset and extended training.\n",
        "\n",
        "## Learning Goals\n",
        "- Implement Transformer encoder and decoder from scratch\n",
        "- Apply masking and padding correctly\n",
        "- Train with Teacher Forcing\n",
        "- Observe epoch-by-epoch loss changes"
      ],
      "metadata": {
        "id": "1bsqNANdz8L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. Environment Setup\n",
        "# ==============================================================================\n",
        "# !pip install numpy==1.26.4\n",
        "# !pip install d2l --no-deps\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from d2l import torch as d2l\n",
        "import random\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. Helper Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def safe_tokenize(sentence_tokens, vocab):\n",
        "    \"\"\"\n",
        "    Converts a list of tokens to indices, falling back to '<unk>' for out-of-vocabulary tokens.\n",
        "    \"\"\"\n",
        "    return [vocab[token] if token in vocab.token_to_idx else vocab['<unk>']\n",
        "            for token in sentence_tokens]\n",
        "\n",
        "def init_weights(module):\n",
        "    \"\"\"\n",
        "    Initializes weights for different layers according to best practices for Transformers.\n",
        "    \"\"\"\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, mean=0, std=0.02)\n",
        "    elif isinstance(module, nn.LayerNorm):\n",
        "        nn.init.ones_(module.weight)\n",
        "        nn.init.zeros_(module.bias)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. Model Definition: Building a BERT-GPT Style Seq2Seq Model\n",
        "# ==============================================================================\n",
        "\n",
        "class BertStyleEncoder(nn.Module):\n",
        "    \"\"\"A BERT-style Transformer Encoder.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = d2l.PositionalEncoding(d_model, dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=num_heads, dim_feedforward=ff_dim,\n",
        "            dropout=dropout, activation='gelu', batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # Scale embedding by sqrt(d_model) as suggested in the \"Attention Is All You Need\" paper.\n",
        "        x = self.embedding(src) * (self.embedding.embedding_dim ** 0.5)\n",
        "        x = self.pos_encoding(x)\n",
        "        return self.encoder(x, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "class GptStyleDecoder(nn.Module):\n",
        "    \"\"\"A GPT-style Transformer Decoder.\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = d2l.PositionalEncoding(d_model, dropout)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model, nhead=num_heads, dim_feedforward=ff_dim,\n",
        "            dropout=dropout, activation='gelu', batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Weight Tying: Share weights between the embedding layer and the final output layer.\n",
        "        self.fc_out.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        x = self.embedding(tgt) * (self.embedding.embedding_dim ** 0.5)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.decoder(\n",
        "            x, memory, tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class TransformerSeq2Seq(nn.Module):\n",
        "    \"\"\"The main Sequence-to-Sequence model combining the encoder and decoder.\"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_layers=6, num_heads=8, ff_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = BertStyleEncoder(src_vocab_size, d_model, num_layers, num_heads, ff_dim, dropout)\n",
        "        self.decoder = GptStyleDecoder(tgt_vocab_size, d_model, num_layers, num_heads, ff_dim, dropout)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
        "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        memory = self.encoder(src, src_mask, src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Inference Function: Beam Search\n",
        "# ==============================================================================\n",
        "\n",
        "def predict_translation_beam_search(model, sentence_tokens, src_vocab, tgt_vocab, device, max_len=20, beam_size=3):\n",
        "    \"\"\"\n",
        "    Performs inference using beam search to generate a translation.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    # Preprocess the source sentence\n",
        "    src_indices = safe_tokenize(sentence_tokens, src_vocab) + [src_vocab['<eos>']]\n",
        "    src_tensor = torch.tensor(src_indices, device=device).unsqueeze(0)\n",
        "    src_key_padding_mask = (src_tensor == src_vocab['<pad>']).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encoder output is calculated only once\n",
        "        memory = model.encoder(src_tensor, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Initialize beams: a list of tuples (sequence, score)\n",
        "        beams = [(torch.tensor([[tgt_vocab['<bos>']]], device=device), 0.0)]\n",
        "\n",
        "        # Autoregressive decoding loop\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "            for seq, score in beams:\n",
        "                # If a beam has already ended, add it to the results and continue\n",
        "                if seq[0, -1].item() == tgt_vocab['<eos>']:\n",
        "                    new_beams.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare decoder inputs\n",
        "                tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq.size(1)).to(device)\n",
        "\n",
        "                # Get model output and log probabilities\n",
        "                out = model.decoder(seq, memory, tgt_mask=tgt_mask, memory_key_padding_mask=src_key_padding_mask)\n",
        "                log_probs = torch.log_softmax(out[:, -1, :], dim=-1)\n",
        "\n",
        "                # Get the top k candidates for the next token\n",
        "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=-1)\n",
        "\n",
        "                # Expand the beam with new candidates\n",
        "                for k in range(beam_size):\n",
        "                    next_seq = torch.cat([seq, topk_indices[:, k].unsqueeze(0)], dim=1)\n",
        "                    next_score = score + topk_log_probs[0, k].item()\n",
        "                    new_beams.append((next_seq, next_score))\n",
        "\n",
        "            # Prune the beams: keep only the top `beam_size` best sequences\n",
        "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "            # Stop if the best beam has reached the end token\n",
        "            if beams[0][0][0, -1].item() == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "    # Post-process the best sequence to get the final translation\n",
        "    best_seq_indices = beams[0][0][0].tolist()\n",
        "    # Remove <bos> and <eos> tokens\n",
        "    best_seq_indices = best_seq_indices[1:]\n",
        "    if tgt_vocab['<eos>'] in best_seq_indices:\n",
        "        best_seq_indices = best_seq_indices[:best_seq_indices.index(tgt_vocab['<eos>'])]\n",
        "\n",
        "    return ' '.join([tgt_vocab.to_tokens(idx) for idx in best_seq_indices])\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Training and Execution\n",
        "# ==============================================================================\n",
        "\n",
        "# Load the dataset (e.g., English to French translation)\n",
        "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size=64, num_steps=20)\n",
        "\n",
        "def train_loop(model, data_iter, lr, num_epochs, tgt_vocab, device, src_vocab):\n",
        "    \"\"\"The main training loop for the Seq2Seq model.\"\"\"\n",
        "    # Use CrossEntropyLoss, ignoring the padding token index for loss calculation.\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_vocab['<pad>'])\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set model to training mode\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch in data_iter:\n",
        "            src, _, tgt, _ = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Prepare decoder input (teacher forcing) and target\n",
        "            dec_input = tgt[:, :-1]  # Exclude the last token (<eos>)\n",
        "            dec_target = tgt[:, 1:]   # Exclude the first token (<bos>)\n",
        "\n",
        "            # Generate necessary masks\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(dec_input.size(1)).to(device)\n",
        "            src_key_padding_mask = (src == src_vocab['<pad>']).to(device)\n",
        "            tgt_key_padding_mask = (dec_input == tgt_vocab['<pad>']).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, dec_input,\n",
        "                           tgt_mask=tgt_mask,\n",
        "                           src_key_padding_mask=src_key_padding_mask,\n",
        "                           tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                           memory_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(output.reshape(-1, output.shape[-1]), dec_target.reshape(-1))\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        avg_loss = total_loss / batch_count\n",
        "        print(f\"Epoch {epoch+1}, Loss {avg_loss:.4f}\")\n",
        "\n",
        "        # ----- Display a translation example after each epoch -----\n",
        "        test_sentence_en = \"i ate a sandwich .\".split()\n",
        "        translated_sentence = predict_translation_beam_search(model, test_sentence_en, src_vocab, tgt_vocab, device)\n",
        "        print(f\"  Input (en): {' '.join(test_sentence_en)}\")\n",
        "        print(f\"  Translation (fr): {translated_sentence}\\n\")\n",
        "\n",
        "# ----- Main Execution Block -----\n",
        "if __name__ == '__main__':\n",
        "    device = d2l.try_gpu()\n",
        "    model = TransformerSeq2Seq(len(src_vocab), len(tgt_vocab))\n",
        "    model.apply(init_weights)\n",
        "    train_loop(model, train_iter, lr=0.001, num_epochs=100,\n",
        "                 tgt_vocab=tgt_vocab, device=device, src_vocab=src_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMK7jKwIx7br",
        "outputId": "77bfa239-396b-4ad2-e535-42c0db93ae0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss 8.2201\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis suis\n",
            "\n",
            "Epoch 2, Loss 3.4390\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 3, Loss 2.9938\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 4, Loss 2.9396\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 5, Loss 2.9198\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 6, Loss 2.8560\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 7, Loss 2.8916\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 8, Loss 2.9152\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 9, Loss 2.8922\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 10, Loss 2.8805\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 11, Loss 2.8835\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 12, Loss 2.8621\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 13, Loss 2.8802\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 14, Loss 2.8877\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 15, Loss 2.8726\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 16, Loss 2.8779\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 17, Loss 2.8882\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 18, Loss 2.8676\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 19, Loss 2.8773\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 20, Loss 2.8675\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 21, Loss 2.8743\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 22, Loss 2.8797\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 23, Loss 2.8810\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 24, Loss 2.8510\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 25, Loss 2.8829\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 26, Loss 2.8387\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 27, Loss 2.8618\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 28, Loss 2.8632\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 29, Loss 2.8771\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 30, Loss 2.8626\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 31, Loss 2.8741\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 32, Loss 2.8724\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 33, Loss 2.8485\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 34, Loss 2.8856\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 35, Loss 2.8602\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 36, Loss 2.8741\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 37, Loss 2.8787\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 38, Loss 2.8731\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 39, Loss 2.8555\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 40, Loss 2.8820\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 41, Loss 2.8511\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 42, Loss 2.8502\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 43, Loss 2.8208\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 44, Loss 2.8539\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 45, Loss 2.8757\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 46, Loss 2.8576\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 47, Loss 2.8525\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 48, Loss 2.8600\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 49, Loss 2.8389\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 50, Loss 2.8710\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 51, Loss 2.8408\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 52, Loss 2.8664\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 53, Loss 2.8437\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 54, Loss 2.8407\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 55, Loss 2.8627\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 56, Loss 2.8572\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 57, Loss 2.8299\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 58, Loss 2.8162\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 59, Loss 2.7989\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 60, Loss 2.8361\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 61, Loss 2.8280\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 62, Loss 2.8155\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 63, Loss 2.8458\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 64, Loss 2.8712\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 65, Loss 2.8493\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 66, Loss 2.8566\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 67, Loss 2.7729\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 68, Loss 2.8703\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 69, Loss 2.8283\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 70, Loss 2.8569\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 71, Loss 2.8270\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 72, Loss 2.8296\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 73, Loss 2.8102\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 74, Loss 2.7993\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 75, Loss 2.8117\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 76, Loss 2.8086\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 77, Loss 2.7857\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 78, Loss 2.8066\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 79, Loss 2.7774\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 80, Loss 2.7549\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 81, Loss 2.7857\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 82, Loss 2.8169\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 83, Loss 2.7811\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 84, Loss 2.7802\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 85, Loss 2.7806\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 86, Loss 2.7545\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 87, Loss 2.7042\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 88, Loss 2.7141\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 89, Loss 2.6733\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 90, Loss 2.6795\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 91, Loss 2.6696\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "Epoch 92, Loss 2.6232\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 93, Loss 2.6575\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 94, Loss 2.7278\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): \n",
            "\n",
            "Epoch 95, Loss 2.7177\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 96, Loss 2.6580\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "Epoch 97, Loss 2.6779\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 98, Loss 2.6523\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 99, Loss 2.6494\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! !\n",
            "\n",
            "Epoch 100, Loss 2.6143\n",
            "  Input (en): i ate a sandwich .\n",
            "  Translation (fr): <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}