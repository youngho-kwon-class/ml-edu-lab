# 1. 라이브러리 불러오기
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import seaborn as sns

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#  2. 데이터셋 로딩
transform = transforms.ToTensor()
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)

# 3. VAE 모델 정의 (latent dim = 2)
LATENT_DIM = 2

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.enc_conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)
        self.enc_conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)
        self.fc_mu = nn.Linear(64 * 7 * 7, LATENT_DIM)
        self.fc_logvar = nn.Linear(64 * 7 * 7, LATENT_DIM)

        self.fc_dec = nn.Linear(LATENT_DIM, 64 * 7 * 7)
        self.dec_conv1 = nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1)
        self.dec_conv2 = nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1)

    def encode(self, x):
        x = F.relu(self.enc_conv1(x))
        x = F.relu(self.enc_conv2(x))
        x = x.view(x.size(0), -1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        x = F.relu(self.fc_dec(z))
        x = x.view(-1, 64, 7, 7)
        x = F.relu(self.dec_conv1(x))
        x = torch.sigmoid(self.dec_conv2(x))
        return x

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar

# 4. 손실 함수 정의
def vae_loss(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# 5. 학습 루프
model = VAE().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

epochs = 10
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar = model(data)
        loss = vae_loss(recon_batch, data, mu, logvar)
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}")

# 6. 원본 vs 복원 이미지 시각화
model.eval()
with torch.no_grad():
    sample = next(iter(train_loader))[0][:8].to(device)
    recon, _, _ = model(sample)

    fig, axes = plt.subplots(2, 8, figsize=(15, 4))
    for i in range(8):
        axes[0, i].imshow(sample[i].cpu().squeeze(), cmap='gray')
        axes[0, i].axis('off')
        axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')
        axes[1, i].axis('off')
    plt.suptitle("Top: Original | Bottom: Reconstructed")
    plt.show()

# 7. 잠재 공간 시각화
z_list = []
label_list = []

with torch.no_grad():
    for data, labels in train_loader:
        data = data.to(device)
        mu, logvar = model.encode(data)
        z = model.reparameterize(mu, logvar)
        z_list.append(z.cpu())
        label_list.append(labels)

z_all = torch.cat(z_list)
labels_all = torch.cat(label_list)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=z_all[:, 0], y=z_all[:, 1], hue=labels_all, palette='tab10', s=15, alpha=0.7)
plt.title("Latent Space Visualization")
plt.xlabel("z₁")
plt.ylabel("z₂")
plt.legend(title="Digit", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()

#  8. 잠재 공간 샘플링
grid_x = torch.linspace(-3, 3, steps=10)
grid_y = torch.linspace(-3, 3, steps=10)

fig, axes = plt.subplots(10, 10, figsize=(10, 10))
model.eval()

for i, y in enumerate(grid_y):
    for j, x in enumerate(grid_x):
        z = torch.tensor([[x, y]], dtype=torch.float32).to(device)
        with torch.no_grad():
            sample = model.decode(z).cpu().squeeze()
        axes[i, j].imshow(sample, cmap='gray')
        axes[i, j].axis('off')

plt.suptitle("Samples from Latent Space Grid")
plt.tight_layout()
plt.show()

